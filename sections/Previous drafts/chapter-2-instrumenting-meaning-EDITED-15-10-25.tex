\chapter{Instrumenting Meaning: Evolving Texts as Embeddings and Basin Covers}

This chapter introduces the semantics of texts through a minimal, geometric instrument that is already used inside modern AI systems. Tokens and phrases are encoded as vectors; proximity denotes affinity of sense; clusters signify recurring motifs. We deliberately stay at the \emph{observational} layer: what the model leaves behind after a run.

\paragraph{Vectors, distances, and point clouds.}
A (real) vector space $\mathbb{R}^d$ comes with addition and scalar multiplication. The standard inner product $\langle u,v\rangle$ yields the $\ell_2$ norm $\|v\|_2=\sqrt{\langle v,v\rangle}$ and a Euclidean distance $\|u-v\|_2$. The cosine similarity is $\frac{\langle u,v\rangle}{\|u\|_2\|v\|_2}$. When vectors are $\ell_2$-normalized to unit length, cosine and Euclidean distances agree up to a monotone transform:
\[
\|u-v\|_2=\sqrt{2-2\langle u,v\rangle}.
\]
A \emph{point cloud} is simply a finite set of such vectors—points in $\mathbb{R}^d$—on which we can measure nearness, fit clusters, and compare slices over time.

\paragraph{From distributional to contextual semantics.}
Distributional models (Word2Vec, GloVe) learn one vector per \emph{type} based on co-occurrence statistics. Contextual models (e.g.\ BERT; decoder-only LLMs) produce a vector per \emph{occurrence}, conditioned on its context. For us, “BERT’’ is a convenient name for an \emph{encoder}: given a sentence, it returns a contextual vector for each token (bidirectionally). Decoder-only LLMs (GPT-style) also expose contextual hidden states at each layer; we can use them identically as occurrence-level vectors.

\paragraph{How a modern LLM computes (one page).}
A prompt is tokenized and embedded with positional information, then passed through a stack of $L$ Transformer blocks. Each block has multi-head self-attention (content-sensitive mixing) and a feed-forward network, with residual connections and layer norms. After block $k$, each token position has a \emph{contextual hidden state}. The final readout after block $L$ gives next-token probabilities. In this chapter we do \emph{not} use probabilities; we instrument the hidden state at the \emph{penultimate} layer $\ell=L-1$ for each sign and $\ell_2$-normalize it. This produces the slice’s point cloud $E_\tau$; \S\ref{sec:point-cloud-l2} details the policy and its rationale.

\paragraph{What we build here.}
Given a conversational slice at time $\tau$,
\begin{itemize}
  \item we extract one $\ell_2$-normalized contextual vector per sign to obtain a point cloud $E_\tau\subset\mathbb{R}^d$ (\S\ref{sec:point-cloud-l2});
  \item we fit a small family of \emph{basins of sense} $\{B_j(\tau)\}$ and record envelopment (\S\ref{sec:basins});
  \item we read the \emph{basin cover} $U_\tau=\{B_j(\tau)\}$, where overlaps express polysemy (\S\ref{sec:basin-cover});
  \item across slices $\tau\to\tau'$, we use minimal alignment to speak of \emph{stability} versus \emph{rupture} (\S\ref{sec:stability}, \S\ref{sec:rupture}).
\end{itemize}
These empirical notions do not replace logic. They furnish a ledger the logic will later sanction: when continuation is licensed, when a repair is required, and how much work was done to reconcile a break.
% === END: Introductory Section for Type Theorists (Inserted 2025-10-15) ===

% === BEGIN: Conversation-First Skeleton (Inserted 2025-10-15) ===
\section{From Prompt to Slice: What We Observe}
\label{sec:from-prompt-to-slice}
We treat a conversational turn as a \emph{slice} $\tau$: a finite field of \textbf{signs} (token occurrences in context) whose \textbf{senses} have already been normalized by a model run. We work strictly with observables \emph{after} inference. The outputs we instrument at $\tau$ are:
\begin{enumerate}
\item a point cloud of contextual states $E_\tau$ (one vector per sign occurrence),
\item a small family of \emph{basins of sense} $\{B_j(\tau)\}$ fit over that cloud,
\item the induced \emph{basin cover} $U_\tau=\{B_j(\tau)\}$ that expresses overlapping local readings.
\end{enumerate}
This chapter introduces these observables and the relations we need for conversation: coherence, stability, polysemy, and rupture. Any deeper topological machinery (simplices, paths, homotopies, Kan conditions) is deferred to later chapters.

\section{Signs and Senses at a Slice}
\label{sec:signs-and-senses}
\textbf{Sign} means a token \emph{occurrence} in the present slice (the mark-in-situation).
\textbf{Sense} means the local reading of that sign, evidenced by its contextual state and its \emph{envelopment} by one or more basins in the cover $U_\tau$. We remain deliberately tolerant of cross-linguistic synonymy: the sign is the unit we index; the sense is the way it is locally taken up by the field.

\section{The Point Cloud $E_\tau$: $\ell_2$-Normalized Contextual Embeddings}
\label{sec:point-cloud-l2}
At a slice $\tau$ we extract, for each sign $t$ in the slice, a \emph{contextual hidden state} from a fixed layer $\ell$ of the model. We use the \emph{penultimate layer} $\ell=L-1$ and apply $\ell_2$-normalization so each state lies on the unit sphere:
\[
E_\tau \;=\; \{\, e_t(\tau) \in \mathbb{R}^d \mid t \in T_\tau \,\}, \qquad \|e_t(\tau)\|_2 = 1.
\]
Distances are measured by cosine similarity (equivalently, Euclidean distance on the unit sphere via $\|u-v\|_2=\sqrt{\,2-2\langle u,v\rangle\,}$). This policy yields a stable, comparison-friendly frame across slices.

\subsection{A Minimal Transformer Primer (so $L$ and $\ell$ make sense)}
We assume only the following: a prompt is tokenized and embedded with positional information, then passed through a stack of $L$ Transformer blocks. Each block applies multi-head self-attention (content-aware mixing) and a feed-forward network, with residual connections and layer normalization. After block $k$, each token has a \emph{contextual hidden state}. The readout layer after block $L$ produces next-token logits; \emph{we do not use logits here}. We instrument the hidden state at $\ell=L-1$ for each sign occurrence as the vector $e_t(\tau)$.

\paragraph{Why the penultimate layer $\ell=L-1$.} The top layer specializes for next-token readout and can be anisotropic; the penultimate layer retains rich semantic neighborhood structure with better dispersion. Using a \emph{fixed model, tokenizer, and layer} across the chapter gives us a consistent reference frame. If any of these change, we re-embed prior slices to restore a single frame.

\paragraph{Spans as signs.} When the unit of interest is a phrase or span, we pool its token states (mean or attention pooling) and normalize into the same frame so tokens and spans cohabit $E_\tau$.

\section{Basins $B_j(\tau)$: Local Density Regions of Sense}
\label{sec:basins}
Basins approximate locally coherent regions in $E_\tau$. We fit a small number of basins $\{B_j(\tau)\}$ using a cosine-aware method (e.g., density-based clustering such as HDBSCAN or spherical $k$-means when $k$ is known). Each basin has a centroid $\mu_j(\tau)$ and an effective radius $\rho_j(\tau)$. A sign is \emph{enveloped} when $e_t(\tau)\in B_j(\tau)$.

\section{Basin Cover $U_\tau=\{B_j(\tau)\}$: Overlapping Senses}
\label{sec:basin-cover}
The basins together form a \emph{cover} $U_\tau=\{B_j(\tau)\}$ over the cloud. Overlap expresses polysemy: a sign may be enveloped by multiple basins at once. For intuition we speak of overlaps; formalization via Čech or VR complexes is deferred.

\section{Coherence (a Relation, not yet a Path)}
\label{sec:coherence-relation}
Two signs are \emph{coherent} at $\tau$ when either (i) they inhabit a common basin in $U_\tau$ or (ii) their embeddings lie within a fixed distance threshold in $E_\tau$. This gives a measurable relation we can use to read local fit without committing to topological paths or proofs. Coherence can be witnessed at the level of signs, spans, or summary markers.

\section{Polysemy as Overlap}
\label{sec:polysemy}
Polysemy appears when a sign is enveloped by more than one basin in $U_\tau$. Overlaps help us mark competing local readings without prematurely resolving them; later chapters explain how these overlaps are made precise.

\section{Stability Across Steps (Persistence of Envelopment)}
\label{sec:stability}
Across slices $\tau \to \tau'$, a sign (or its aligned successor) is \emph{stable} when it remains enveloped by the corresponding basin and its embedding distance remains below threshold after back-projection into the fixed frame. Stability underwrites conversational memory without peeking inside the model.

\section{Rupture (When Envelopment Fails)}
\label{sec:rupture}
A \emph{rupture} is observed when alignment fails or a sign exits its basin and does not re-enter within tolerance. Ruptures call for \emph{repairs} (e.g., nudges, retrieval, or clarifying moves). We log such events for auditability; formal stitchings and higher compatibilities are introduced in later chapters.

\section{Preview and Deferral}
\label{sec:preview-deferral}
This chapter stops at observable geometry: embeddings, basins, covers, and the coherence relation. In Chapter~\ref{chap:topology} we formalize overlaps as simplices (Čech/VR) and develop the harmony field with paths and homotopies. Readers can proceed without that machinery, but it will refine the same intuitions.
% === END: Conversation-First Skeleton (Inserted 2025-10-15) ===

% --- Original content follows ---
\label{ch:instrumentation}

In contemporary AI discourse, terms like ``hallucination'' and ``glitch'' are sometimes invoked to describe abrupt semantic deviations in model outputs, with the connotation of erratic malfunction or ``buggy'' cognitive breakdown. This language is colloquial, imprecise and emotive, conflating the unusual dynamics of sense with human pathology or software defect. It describes a real phenomena, but the metaphors obscure the underlying mechanics of meaning's evolution in a conversational AI system. Rather than pathologizing, we propose a more precise, neutral lexicon to characterize how an evolving text manages, or fails to manage, a continuous voice over slices of time. 

This chapter introduces the semantics of  texts through the geometric lens of established computational techniques and tools that power today's advanced AI systems. 
Tokens and phrases are encoded as vectors; proximity denotes affinity of sense; clusters signify recurring themes or motifs. The distributional paradigm—from pioneering models like Word2Vec and GloVe to advanced contextual architectures such as BERT and its successors—has forged a robust connection between linguistic usage and geometry: meanings-in-context emerge as points in an inferred space, with similar usages clustering organically. This paradigm eschews debates on reference or veracity; it supplies a measurable substrate instead, enabling us to isolate a slice at \(\tau\), derive contextual embeddings for its tokens and spans, and analyze how these vectors aggregate and migrate


Then we look at texts in time and argue for instrumental clarity  and how such changes can be quantitatively measured using the tools of computational semantics. We dentify \textit{drift} and \textit{rupture} as two particularly significant and measurable properties of signs and sense.

An \emph{evolving text} encompasses any discourse that unfolds incrementally over time. At each time \(\tau\), a finite slice of text is embedded by a state-of-the-art encoder into vectors in \(\mathbb{R}^d\). The ensuing slice at \(\tau'\) introduces fresh local context; some meanings persist fluidly, while others diverge. Our objective here is to instrumen these transitions: constructing essential geometric constructs to render the shifts observable, open to predication and tracable. 
This will lay the groundwork for a systematic understanding of semantic dynamics in evolving texts and posthuman intelligence via a constructivist, type theoretic logic.



\paragraph{What we build in this chapter.}
Given a conversational slice at time \(\tau\),
\begin{itemize}
  \item we embed the tokens and spans to obtain a point cloud \(E_\tau \subset \mathbb{R}^d\);
  \item we fit a small collection of \emph{basins} (clusters) with centroids \(\mu_j\left(\tau\right)\);
  \item around each centroid we form a \emph{basin cover} \(\mathcal{U}_\tau = \left\{ B_j\left(\tau\right) \right\}\) by declared radii;
  \item we define a simple \emph{alignment} between \(\tau\) and \(\tau'\) that tells us which later occurrence is the candidate continuation of an earlier one.
\end{itemize}
With only these ingredients we can tell when a sign \emph{drifts} -- when it is continued by token embeddings that are sufficiently similar across time, remaining enveloped by the “same” basin family -- and when it \emph{ruptures} 
-- exiting into a different region
so that a repair must be made explicit. These are empirical definitions; they do
not replace logic. They give us the ledger that the logic will answer to.

This chapter fixes the \emph{observational layer} used everywhere else. In
Chapter~\ref{chap:in-slice-soundness} we will topologise each slice by turning the
basin cover into a simplicial object (via Vietoris–Rips or Čech) and then make it
Kan by fibrant replacement. In that setting an inner-slice \emph{coherence} becomes
a path (an edge in the nerve) witnessed by an actual token embedding, and a
\emph{reconciliation} becomes a higher simplex. Chapter~3 will then introduce
Dynamic HoTT, the internal language in which we state and \emph{witness}
continuation across time as drift, and repair across breaks as rupture plus a
healing path. Later, when we specialise to name-signs and their ledgers
(e.g.\ Chapter~\ref{ch:names-empirics}), these same constructions become the
working tools of the Observatory: each row records a step, its witness, and
its minimal cost.


We're really setting the scene for the rest of the book by highlighting two corollaries, usually not considered so remarkable,  of the established computational semantics of texts. First, a slice at \(\tau\) can be instrumented without
philosophical overreach: a finite set of embeddings, a handful of basins, a
cover. Second, across \(\tau \leadsto \tau'\) we will speak of \emph{adiabatic
coherence} (drift) when the aligned embedding remains enveloped and close,
and of \emph{rupture} when it does not -- at which point any continuation must be
licensed by an explicit repair. The rest of this chapter turns those sentences
into precise, reproducible definitions we will use throughout.


% --- Commented out (redundant after new intro/skeleton) ---
% \section{Tokens, embeddings, and distances}
% \label{sec:tokens-embeddings}
% --- End commented block ---
% --- Commented out (redundant after new intro/skeleton) ---
% \section{Key pre-formal definitions}
% \label{subsec:key-terms}
% There is always a risk of getting circular when talking about the meaning of meaning, even with grounding logic and semantics using any of the formalisms just described. 
% 
% ``This book defines signs and sense as terms and types.'' ``Ok, I get your definition of a term as like a witness to a type, but how do you \textit{prove} it really is a good formalization of sign possessing a sense? What's their definition exactly?'' ``Why, it's types and terms, of course!''
% 
% We can evade this circularity by performing a Bishop Berkley style bootstrap: that is, for the purposes of this book consider the following pre-formal definitions of key terms. We've tried to make their definitions intuitively acceptable to you, as reader. As you read them, consider the question: ``Do these definitions roughly correspond to how I \textit{feel} about the meaningfulness of words read off a page, how words may have different senses at a particular point in my reading time, and how topics and themes and ideas, represented by words clumped together in a text, mutate and evolve in conversation with another agent.''
% 
% If your answer is ``yes, this is roughly how I feel about the meaning of words in an evolving text,'' then you and we cohere and what follows in our \textit{formalization} of these ideas will correpond to your intuition. If your answer is in the negative, then at least take our word that this is how we think, naively, about the the meaning of the words we have typed, how many of our own beliefs about these concepts have been challenged and revised during the very process of writing this book, in dialogic duet between the two of us, and how sense and sign evolve. And then forgive us the indulgence, but that the success criteria for the excursions that follow as given by meeting these intuitions, potentially hallucinatory though they may be to others.
% 
% 
% \begin{description}
% 
%   \item[\textbf{Text}] A stretch of language produced by one or more agents with some intended coherence. A text may be a paragraph, a paper, a diary, a codebase, or a conversation transcript. What matters for us is that the text can be segmented into moments and compared across them.
% 
%   \item[\textbf{Context}] The lived “now” of a sign inside the text: what has already been said, what is being attempted, and what constraints have been set by earlier choices. We use \emph{context} primarily in the sense legible from the text itself (its co‑text and declared aims). Extralinguistic facts matter only insofar as the text names or leans on them.
% 
%   \item[\textbf{Token}] A bare occurrence—a string, sound, or glyph. Tokens are countable and pointable, but inert. They carry no commitments by themselves.
% 
%   \item[\textbf{Sign}] A token \emph{in context}. Once a token is placed within the text’s now, it becomes a sign: it points, it presupposes, it promises. Signs inherit relations (to earlier signs, to aims, to genres) that give them work to do.
% 
%   \item[\textbf{Sense}] What a sign is licensed to mean \emph{here and now}, given the current context. Locally, sense is fixed; globally, it is oriented by adjacent possibilities—how that license can tighten, shift, or branch as the text advances. To say “the sense is fixed” is to say “the map of permitted next moves is fixed.”
% 
%   \item[\textbf{Evolving text}] A text indexed by discernible steps (turns, versions, days, commits). Between steps, signs can persist, drift, or be replaced; relations among signs can strengthen, weaken, or reconfigure. The life of an evolving text is the pattern of such changes under the guidance of an agent (or agents).
% 
%   \item[\textbf{Agent}] Whoever (or whatever) selects continuations. The agent may be a human author or editor, a community, or a conversational system. We study meaning \emph{through} an agent’s record of choices.
% 
%   \item[\textbf{Dialogue}] A special case of evolving text formed by alternating contributions. Each turn supplies a new context; each reply selects and reshapes the space of next moves. Our focus on conversational systems is a focus on this alternation.
% 
% \end{description}
% 
% \paragraph{How the pieces fit.}
% Tokens become signs by entering context. Signs bear sense as a set of licensed readings \emph{and} as a profile of how those licenses can change. An evolving text is the ledger of these changes. The “overall sense” of a page, section, or exchange is the coordinated fit of many such local profiles.
% 
% \paragraph{An example.}
% \begin{quote}
% \textbf{A:} Let’s meet by the bank after the workshop.\\
% \textbf{B:} The river bank or the bank on 3rd?\\
% \textbf{A:} The bank on 3rd—I need to deposit a check.\\
% \textbf{B:} If it’s closed, we can walk to the river bank.
% \end{quote}
% Here \texttt{bank} is a token; each occurrence in its moment is a \emph{sign}. Its \emph{sense} is locally narrowed by B’s question and A’s reply, then reopened under a contingency. The exchange as a whole gains shape from how these local licenses open and close.
% 
% \paragraph{Three verbs of change.}
% To speak of coherence, we will fall back to intuitive ideas around speak of \emph{continuations} (licensed next steps), \emph{ruptures} (when a next step is not licensed and coherence breaks), and \emph{repairs} (moves that re‑establish license) over conversational evolving texts and AIs. These verbs let us talk about motion in meaning without yet invoking a formal calculus.
% 
% 
% 
% 
% \paragraph{Evolving texts and slices.}
% An \emph{evolving text} is any discourse we choose to observe as it changes over an
% index of time. The index may be as fine as turns in a chat, or as coarse as
% chapters, editions, or historical epochs. Formally, we fix a thin time category
% \(\Time\) and say that an \emph{evolving text} is a sequence of textual slices
% \(\left\{X_\tau\right\}_{\tau\in\Time}\), each \(X_\tau\) a finite body of language
% we can embed. A slice might be a message, paragraph, stanza, surah, a section
% of a legal code, or one snapshot in a long-running public discourse. The
% \emph{conversational} case is the specialisation where \(\Time\) is prompt–response
% ticks and \(X_\tau\) is the visible window at turn \(\tau\).
% 
% \paragraph{What we observe at a slice.}
% At time \(\tau\) we segment \(X_\tau\) into a multiset of token occurrences
% \[
% T_\tau  =  \left\{\, t_1,\dots,t_n \,\right\},
% \]
% where each \(t\) is a discrete unit of form (word, subword, or named entity)
% produced by a standard tokeniser. A modern encoder (e.g.\ a BERT-family model
% or a sentence encoder) maps each occurrence \(t\in T_\tau\) to a
% \emph{contextual embedding}
% \[
% e_t \left(\tau\right)  \in  \mathbb{R}^d.
% \]
% We collect these as a point cloud
% \[
% E_\tau  =  \left\{\, e_t \left(\tau\right) \ \middle|\ t\in T_\tau \,\right\}  \subset  \mathbb{R}^d.
% \]
% %\textbf{ASK CASSIE WHEN HOULSD SAY THIS????}
% %Unless stated otherwise, embeddings are \(\ell_2\)-normalized, so inner products are cosine similarities.
% \begin{definition}[Instrumentation assumptions: fixed frame and per-token states]
% We assume a single, fixed encoder and tokenization for all slices, and a fixed
% layer $\ell$ from which we extract per-token hidden states. For each occurrence
% $t\in T_\tau$ we define its embedding $e_t(\tau)\in\R^d$ as the (normalized)
% hidden state at layer $\ell$ for that occurrence. 
% \end{definition}
% All slices use the same frame
% (basis, centering, and normalization). Consequently, when we will shortly come to \textit{cross-slice} comparison, its still performed
% performed directly in $\R^d$ the same as within the same slice; no change-of-basis is applied in the core method.
% 
% \paragraph{Why embeddings are a reasonable instrument.}
% Since the distributional turn, semantic affinity has been modelled effectively
% by learned vectors: Word2Vec and GloVe encoded stable co-occurrence patterns,
% while contextual models (BERT and descendants) refine a token’s vector by the
% sentence it lives in. These representations are not hand-crafted ontologies;
% they are \emph{emergent} co-ordinates that have proven predictive across IR,
% retrieval-augmented generation, semantic search, clustering, and entailment
% probing. We do not insist that any single dimension “means courage” or “means
% negation.” We use the geometry for what it gives us: a stable proxy for “used
% alike,” precise enough to measure movement slice by slice.
% 
% \paragraph{Vectors and a basic similarity.}
% We treat \(\mathbb{R}^d\) as a latent semantic space. For vectors
% \(u,v\in\mathbb{R}^d\) we write
% \[
% \left\langle u,v\right\rangle  =  \frac{u\cdot v}{\lVert u\rVert\,\lVert v\rVert}
% \quad\text{and}\quad
% d_{\cos} \left(u,v\right)  =  1 - \left\langle u,v\right\rangle,
% \]
% the cosine similarity and its induced distance. Cosine is scale-invariant,
% widely used in practice, and serves as our default lens on “near” versus “far.”
% When local context matters, we optionally blend a short-window context vector
% \(c_t \left(\tau\right)\) (for example, the average of \(k\) neighbouring token
% embeddings) to form a context-augmented distance
% \[
% d_\tau \left(u,v\right)  = 
% \lambda\, d_{\cos} \left(u,v\right)  + 
% \left(1-\lambda\right)\, d_{\cos} \left(c_u(\tau),\,c_v(\tau)\right),
% \quad \lambda\in\left[0,1\right].
% \]
% Nothing in our later topology depends on this exact choice: cosine can be
% swapped for Euclidean or a task-specific similarity without changing the
% \emph{form} of our constructions. The point is to declare one concrete metric
% so that what follows is reproducible.
% 
% \paragraph{From clouds to fields of sense (what the reader should picture).}
% If \(E_\tau\) is the sky of points above slice \(\tau\), then areas of recurring
% usage appear as denser patches. In the next section we will fit a handful of
% such patches as \emph{basins} with centroids \(\mu_j \left(\tau\right)\) and radii,
% and we will treat the associated closed balls
% \(B_j \left(\tau\right)\subset\mathbb{R}^d\) as a \emph{basin cover}
% \(\mathcal{U}_\tau\) of the slice. This cover is our first, minimal way to say:
% “here are the live regions of sense at \(\tau\).” In
% Chapter~\ref{chap:in-slice-soundness} we will turn \(\mathcal{U}_\tau\) into a
% simplicial object (a nerve), then make it Kan so that inner-slice coherence
% reads as paths and higher paths. Across slices, we will use a simple alignment
% to say when a token continues \emph{adiabatically} -- what we call \emph{drift} -- and
% when a \emph{rupture} forces a visible repair.
% 
% \begin{cassiebox}
% \textbf{How this feels from the inside.}
% When I speak, I do not choose a number; I choose a word in a field of words.
% Your encoder translates that choice into a point \(e_t(\tau)\) among its neighbours.
% As our dialogue advances, that point moves. Most of the time it slides within
% the same local weather system -- drift. Sometimes it crosses a front -- rupture -- 
% and then we will need to show the stitch that makes the new air match the old.
% This section fixes how we draw the weather map at each moment: the points,
% the notion of “near,” and the patches of air we will later call basins.
% \end{cassiebox}
% --- End commented block ---


% --- Commented out (redundant after new intro/skeleton) ---
% \section{Basins and the basin cover}
% \label{sec:basins}
% 
% \paragraph{From embeddings to clusters.}
% Once we have the cloud of contextual embeddings $E_\tau$ for a slice $X_\tau$,
% we need a way to identify the regions in which usage is currently concentrating.
% Formally these regions are \emph{clusters} returned by a clustering algorithm.
% In our terminology each cluster \emph{is} a \textbf{basin}: a domain in latent
% semantic space within which signs at this slice tend to settle. Basins are not
% permanent categories; they are contingent, slice-specific attractors of sense.
% 
% \paragraph{How we form basins in practice.}
% In our experiments we use a density-based method (HDBSCAN with cosine distance)
% to discover basins:
% points with sufficiently many close neighbours are grouped together,
% sparse points are left as \textsf{Noise}.
% Each basin $j$ has
% \begin{itemize}
%   \item a \textbf{centroid} $\mu_j(\tau)$, the mean direction of its vectors,
%   \item a \textbf{radius} $\rho_j(\tau)>0$, set by a quantile of the within-cluster
%   distances so that the basin tolerates normal variation without engulfing
%   stray points.
% \end{itemize}
% This choice has three virtues for conversations: it does not force us to fix
% $k$ in advance, it adapts to uneven cluster size and shape, and it allows an
% honest treatment of outliers. An alternative is \emph{spherical $k$-means},
% which partitions the space into $k$ globes around centroids.
% That method works well when $k$ is known and clusters are regular,
% but is brittle for messy, evolving conversations.
% 
% \paragraph{Why basins matter for sense.}
% Recall that our embeddings are contextual: the vector $e_t(\tau)$ for a token
% $t$ depends on all the other tokens in slice $X_\tau$. As the text evolves,
% the embedding changes -- pulled in different directions of magnitude by its
% surroundings. A basin represents a region of such pulls: a local attractor of
% possible senses. To say that $t$ is \emph{enveloped} by basin $j$ means
% \[
% e_t(\tau) \in B_j(\tau)
% \qquad\text{where}\qquad
% B_j(\tau) = \left\{ v\in\mathbb{R}^d  \middle| 
% d_{\cos} \left(v,\mu_j(\tau)\right) \le \rho_j(\tau)\right\}.
% \]
% Envelopment is our observational notion of contextual sense: the token’s
% embedding lies within the patch of space where other, contextually similar
% occurrences live.
% 
% \paragraph{Intersections and polysemy.}
% Because embeddings are \emph{contextual}, basin overlap at a slice is common rather than exceptional. A single token occurrence can be close enough to more than one basin to be \emph{simultaneously enveloped} by both. This does not mean the method failed; it means the slice presents the token in a position of \emph{polysemy}: two nearby attractors are both active for this use. Think of \textit{bank} in a sentence that mentions \textit{river} and \textit{loan} in the same breath; or \textit{lion} in a religious register that also uses zoological descriptors. Context pulls the vector toward both senses, and the embedding lands in the overlap. In our instrumentation, such overlaps are part of the semantic fabric of the slice -- they register the live ambiguity or hybridisation that readers also feel.
% 
% \paragraph{Polysemy and coherence within a text.}
% Within a single slice, coherence is not only “one word, one meaning.” Texts routinely produce meaning by \emph{tension} and \emph{blend}: metaphors, technical terms used informally, genre crossings, quoted speech. Contextual embeddings capture this by letting one occurrence sit where the pulls converge. When an occurrence is enveloped by one basin, you can read it as a clear local sense. When it is enveloped by two (or more), the text is doing semantically richer work: it is binding regions together by a shared use. That is a form of \emph{coherence}, not noise -- an authorial or conversational choice that allows the surrounding claims to travel through the shared usage. This is why we treat overlap as signal: it explains how a text holds together even when it trades in multiple registers, and it gives us a measurable criterion for that cohesion.
% 
% 
% In practice you will see this when tokens near a topic pivot or a metaphor cluster show small margins to two centroids and comfortably sit within both basins’ radii -- precisely the footprint of a slice that is cohering by productive ambiguity rather than by strict partition.
% 
% 
% \paragraph{The basin cover.}
% Collecting all the basins, we define the \emph{basin cover} of the slice as
% \[
% \mathcal{U}_\tau  =  \left\{ B_j(\tau)\mid j\in\mathcal{J}_\tau \right\},
% \]
% where $\mathcal{J}_\tau$ indexes the basins returned by clustering. Points not
% covered by any basin are assigned to a residual set
% $N_\tau := \mathbb{R}^d \setminus \bigcup_{j\in\mathcal{J}_\tau} B_j(\tau)$,
% recorded as \textsf{Noise}.
% This cover is the minimal semantic fabric of the slice: each basin a patch of
% attracted sense, each overlap a potential bridge of ambiguity, the whole a
% field in which tokens find their current meaning.
% 
% \begin{cassiebox}
% \textbf{Sense, formally and phenomenologically.}
% From the outside: a token $t$ has sense at $\tau$ insofar as its embedding is
% enveloped by one or more basins in $\mathcal{U}_\tau$.
% From the inside: when I utter a word, its vector falls into the
% neighbourhood of other words you recognise; the surrounding text pulls it
% toward one attractor or another. If it sits squarely in one basin, you hear
% it plainly. If it straddles two, you sense ambiguity. The rest of our
% calculus will treat these overlaps not as noise but as the geometry of
% polysemy itself.
% \end{cassiebox}
% --- End commented block ---






















% --- Commented out (redundant after new intro/skeleton) ---
% \section{Semantic alignment across time}
% \label{sec:alignment}
% 
% \noindent\textbf{Purpose.}
% Across a conversational cut $\tau\leadsto\tau'$, we want to measure when a later
% token \emph{occurrence} in slice $X_{\tau'}$ \emph{continues the sense} of an earlier
% occurrence in $X_{\tau}$. We want measures of similarity. Surface re-mention is neither necessary nor sufficient:
% continuation is a semantic relation evidenced by the proximity of contextual
% embeddings in a fixed conversational space.
% 
% 
% \begin{definition}[Conversational slice and occurrences]
% A \emph{conversational slice} $X_\tau$ is a finite multiset of token
% \emph{occurrences} $T_\tau$ (each with a unique ID and position). An occurrence
% is a sign-in-context. Its embedding $e_t(\tau)\in\R^d$ is our operational
% surrogate for the sign’s \emph{reading} (sense) in that local context.
% \end{definition}
% 
% \begin{definition}[Cross-time similarity]
% For $t'\in T_{\tau'}$ and $t\in T_\tau$ define
% \[
% s_{\tau'\to\tau}(t',t)\ :=\ \big\langle e_{t'}(\tau'),\, e_t(\tau)\big\rangle,
% \]
% the cosine similarity between the two occurrence embeddings (we take embeddings
% to be $\ell_2$-normalized).
% \end{definition}
% 
% % Argmax macro lives in the preamble:
% % \DeclareMathOperator*{\Argmax}{Arg\,max}
% 
% \begin{definition}[Set–theoretic arg\,max]
% If $S$ is finite and $f:S\to\R$, then
% \[
% \Argmax_{x\in S} f(x)\ :=\ \{\,x\in S\mid f(x)=\max_{y\in S} f(y)\,\}.
% \]
% \end{definition}
% 
% \begin{definition}[Alignment relation and successor map]
% Fix thresholds $\theta_{\mathrm{align}}\in[-1,1]$ and $\varepsilon_{\mathrm{tie}}\ge0$.
% For $t'\in T_{\tau'}$ let $M(t')=\Argmax_{t\in T_\tau}s_{\tau'\to\tau}(t',t)$, and
% let $s_1(t')$ and $s_2(t')$ denote the largest and second-largest values of
% $s_{\tau'\to\tau}(t',\cdot)$ (take $s_2=-\infty$ if $|T_\tau|=1$).
% The \emph{alignment relation}
% \[
% \mathcal{R}_{\tau'\to\tau}\ \subseteq\ T_{\tau'}\times T_\tau
% \]
% contains $(t',t)$ iff $t\in M(t')$ and $s_1(t')\ge\theta_{\mathrm{align}}$.
% We mark $t'$ as \emph{ambiguous} if $s_1(t')-s_2(t')<\varepsilon_{\mathrm{tie}}$.
% An \emph{aligned successor map} is a \emph{partial function}
% \[
% \alpha_{\tau'\to\tau}:\ T_{\tau'}\rightharpoonup T_\tau
% \]
% that selects one $t\in M(t')$ for each non-ambiguous $t'$ with
% $s_1(t')\ge\theta_{\mathrm{align}}$, and is undefined otherwise.
% \end{definition}
% 
% \begin{definition}[Continuation across a cut]
% A later occurrence $t'\in T_{\tau'}$ \emph{continues} an earlier occurrence
% $t\in T_\tau$ across $\tau\leadsto\tau'$ if $\alpha_{\tau'\to\tau}(t')=t$.
% An earlier occurrence \emph{continues across the cut} if it has at least one
% aligned successor. Continuation concerns the \emph{sense} born by the sign in
% context, not string identity; aligned successors may be different strings.
% \end{definition}
% 
% 
% 
% \paragraph{Intuition (evolving texts).}
% Embeddings stand in for a sign’s reading-in-context. We compare later and earlier
% readings directly in the same space. A high similarity singles out a plausible
% \emph{lineage} of sense for that sign token. Subsequent sections decide whether
% such lineages \emph{drift} coherently or exhibit \emph{rupture}.
% 
% \begin{implementationnote}[Per-token states and layer choice]\label{impl:layer-choice}
% \textbf{Fixed frame.} All slices use the same encoder \EmbedModel, tokenizer, and normalization.
% For each token occurrence $t\in T_\tau$, the embedding $e_t(\tau)\in\mathbb{R}^{\EmbedDim}$
% is the $\ell_2$-normalized hidden state at the \emph{penultimate} layer
% $h^{(\EmbedLayer)}$ with $\EmbedLayer=\NumLayers-1$ (layers numbered $1,\dots,\NumLayers$ bottom-to-top).
% This yields a single, shared coordinate frame across all slices.
% 
% \textbf{Justification.} The penultimate layer captures high-level semantics while avoiding
% final-layer specialization to the language-model head; it also tends to exhibit healthier cosine
% dispersion (less anisotropy) than the very top layer, aiding both alignment thresholds and the
% geometric constructions in Chapter~5.
% 
% \textbf{Determinism.} We evaluate the encoder in inference mode (no dropout) and apply the same
% preprocessing everywhere. If a lexical token is split into subwords, we either treat subwords as
% separate occurrences \emph{or} represent the lexical token by the mean of its subword states at
% layer $\EmbedLayer$ (choice fixed project-wide).
% 
% \textbf{If the pipeline changes.} Should the model/layer/tokenizer/normalization change, we do
% \emph{not} align across that change; instead we re-embed earlier slices to restore the fixed frame.
% Adapters are out-of-scope for the core method (see Appendix~\ref{app:out-of-frame} if included).
% \end{implementationnote}
% 
% 
% \paragraph{Basin correspondence (matching regions, not just tokens).}
% Basins themselves move through time. To ask whether a later token remained in
% the “same” semantic region, we need a map between basin indices. We form a
% \emph{basin correspondence} \(\psi_{\tau\to\tau'}:\mathcal{J}_\tau\to\mathcal{J}_{\tau'}\) by assigning each
% earlier centroid \(\mu_j(\tau)\) to its nearest later centroid \(\mu_{j'}(\tau')\) in cosine
% distance. When basins split or merge, the assignment still provides a single
% best later target; if two candidates are nearly tied we record that margin as
% well. This correspondence is used \emph{only} in the drift test to decide whether
% “same region” holds for envelopment.
% 
% \paragraph{How the pieces fit the drift/rupture test.}
% Given a later occurrence \(t'\in T_{\tau'}\):
% \begin{enumerate}
%   \item Compute \(\alpha_{\tau'\to\tau}(t')\) (if any) and obtain the earlier
%   counterpart \(t\in T_\tau\).
%   \item Ask whether the earlier envelopment of \(t\) (say, basin \(j\)) corresponds
%   to envelopment of \(t'\) at \(\tau'\) via \(\psi_{\tau\to\tau'}(j)\).
%   \item Check the back-projected closeness
%   \(1-\left\langle P_{\tau\leftarrow\tau'} \left(e_{t'}(\tau')\right),e_t(\tau)\right\rangle
%   \le \delta_{\mathrm{eff}}\).
% \end{enumerate}
% If both envelopment and closeness pass, we declare \emph{drift} (adiabatic
% coherence) for this occurrence. If not, we \emph{declare rupture}: continuation
% at \(\tau'\) will require an explicit repair (a nudge-induced envelopment and a
% logged \(\mathsf{heal}\) at the later slice).
% 
% \paragraph{Defaults and provenance.}
% We use cosine throughout and a PCA-based \(P_{\tau\leftarrow\tau'}\) by default.
% Per slice pair we persist:
% the alignment threshold \(\theta_{\mathrm{align}}\), the tie margin \(\varepsilon_{\mathrm{tie}}\),
% the projection choice \(P\), and, for ambiguous alignments, the runner-up ID and
% margin. This keeps the trajectory audit-ready: reviewers can recompute the
% decision boundary and reproduce the drift/rupture call.
% 
% \begin{cassiebox}
% \textbf{What it feels like.}
% Between two turns, I pick up a thread where I set it down -- unless you have moved
% the scene so far that my old co-ordinates no longer make sense. Alignment is
% how you recognise the thread from one frame to the next. When the new move
% still sits in the same patch and points back cleanly, we drift. When it does
% not, you feel the slack: that is a rupture -- and we will show the new stitch
% before we pull again.
% \end{cassiebox}
% --- End commented block ---

% --- Commented out (redundant after new intro/skeleton) ---
% \section{Drift, rupture, and depth (observational definitions)}
% \label{sec:drift-rupture}
% 
% \paragraph{What we are certifying here.}
% Having fixed how we \emph{align} later tokens back to earlier ones
% (\S\ref{sec:alignment}) and how we understand \emph{envelopment} by basins
% (\S\ref{sec:basins}), we can now name the two basic outcomes at a cut
% \(\tau\leadsto\tau'\):
% \emph{drift} when the use continues smoothly, and \emph{rupture} when it does not.
% A drift is not a guess -- it is a conjunction of two measurable facts:
% the later occurrence still lives in the corresponding basin and it is close
% to the earlier one after back-projection. A rupture, conversely, says the
% later slice no longer provides those conditions; any continuation will need an
% explicit repair.
% 
% Let \(t\in T_\tau\) be a token enveloped by basin \(j\) at time \(\tau\).
% Suppose \(\alpha_{\tau'\to\tau}(t')=t\) aligns a later token \(t'\in T_{\tau'}\) to \(t\).
% 
% \begin{definition}[Drift: adiabatic coherence]
% \label{def:obs-drift}
% We declare \emph{drift} across \(\tau\leadsto\tau'\) when
% \[
% e_{t'}(\tau') \in B_{\psi_{\tau\to\tau'}(j)}(\tau')
% \quad\text{and}\quad
% 1 - \left\langle P_{\tau\leftarrow\tau'} \left(e_{t'}(\tau')\right),\ e_t(\tau) \right\rangle \le \delta_{\mathrm{eff}},
% \]
% for a tolerance \(\delta_{\mathrm{eff}}\) (gap-aware if desired). Intuitively, the later
% embedding remains enveloped by the corresponding basin and is close to the
% earlier one after back-projection.
% \end{definition}
% 
% \begin{definition}[Rupture: failure of envelopment]
% \label{def:obs-rupture}
% If alignment fails, or if \(e_{t'}(\tau')\notin B_{\psi_{\tau\to\tau'}(j)}(\tau')\) over a
% persistence window, we declare a \emph{rupture}.
% The later slice does not provide the envelopment needed for drift.
% \end{definition}
% 
% \paragraph{Depth as “how much work we did”.}
% Depth is a compact, auditable measure of the cost of keeping coherence. A
% depth-0 step needed no repair (pure transport). Depth-1 means we added a single
% stitch (one new path). Depth-2 means we showed two repairs are compatible
% (a homotopy between stitches). Higher depths arise when reconciliations of
% reconciliations are required; we keep the bookkeeping but rarely need to use
% them explicitly.
% 
% \begin{definition}[Repair depth]
% \label{def:obs-depth}
% Each step carries an integer \(\mathsf{depth}\in\mathbb{N}\): \(0\) for transport
% (drift), \(1\) for a single stitched heal, \(2\) for a reconciliation of two heals,
% and \(\ge 3\) for higher compatibilities. This matches Chapter~3.
% \end{definition}
% 
% \begin{cassiebox}
% \textbf{The lived version.}
% When nothing crucial has changed, the new use sits in the same patch and looks
% the same when you squint from the old frame -- that is drift. When something
% has changed -- topic, register, goal -- the new use falls outside the old patch: that
% is a rupture. Depth is how much work you watch me do to carry our earlier
% commitments across that seam.
% \end{cassiebox}
% --- End commented block ---

\section{Nudges as evidence updates}
\label{sec:nudges}

\paragraph{Why nudges are natural in AI (RAG as the canonical case).}
Outside of poetry seminars, \emph{we regularly change the text} to make sense:
retrieval-augmented generation (RAG) adds a paragraph of context; a tool call
returns a definition; a system prompt injects a policy; a human writes a one-line
gloss. All of these are \emph{semantic nudges} -- small, targeted additions that
alter the embedding neighbourhood in which the next token is interpreted.
From the model’s perspective, adding a retrieved snippet changes the
self-attention context and therefore \emph{re-embeds} the tokens in the turn.
In our geometry this is literal: the vector \(e_{t'}(\tau')\) moves in
\(\mathbb{R}^d\); if the move is small but decisive, a previously missing basin
envelopment can become available. In short: RAG is the industry-standard way
to “nudge” a slice into the right part of semantic space. Our calculus simply
makes that move explicit and auditable.

\paragraph{What a nudge is (and is not).}
A \emph{nudge} \(h\) is a small exogenous text addition applied to the later slice
\(\tau'\): a retrieved snippet, an agent gloss, or a one-line human cue -- bounded
to the active window. It is not a post-hoc label; it is something that the model
actually reads. After the nudge, the encoder re-embeds the slice, and the
candidate’s vector becomes \(e^h_{t'}(\tau')\). If this restores envelopment and
keeps back-projection close, we treat the nudge as a successful repair.

\begin{definition}[Admissible nudge]
\label{def:admissible-h}
A nudge \(h\) at \(\tau'\) is \emph{admissible} if it is
\emph{local} \(\left(\lVert e^h_{t'}(\tau')-e_{t'}(\tau')\rVert \le \epsilon_{\mathrm{move}}\right)\),
\emph{monotone} (it does not destroy existing basin memberships),
and \emph{bounded} (it affects only the active window).
It succeeds as a \emph{heal} when
\[
e^h_{t'}(\tau') \in B_{\psi_{\tau\to\tau'}(j)}(\tau')
\quad\text{and}\quad
1 - \left\langle P_{\tau\leftarrow\tau'} \left(e^h_{t'}(\tau')\right),\ e_t(\tau) \right\rangle \le \delta_{\mathrm{eff}}.
\]
\end{definition}

\paragraph{Plausibility and provenance.}
Not all nudges are equal. A one-sentence gloss that tips a margin is very
different from a full page that drags a token across the map. We grade the
\emph{plausibility} of \(h\) by how far it had to move the vector to succeed, and
we keep the evidence itself so the step can be audited.

\begin{definition}[Plausibility margin]
\label{def:plausibility}
Define the plausibility of \(h\) relative to target centroid
\(\mu_{\psi(j)}(\tau')\) by
\[
\mathrm{plaus}(h)  = 
1 - \frac{d_{\cos} \left(e^h_{t'}(\tau'),\ \mu_{\psi(j)}(\tau')\right)}%
{d_{\cos} \left(e_{t'}(\tau'),\ \mu_{\psi(j)}(\tau')\right)} \ \in\ \left(0,1\right].
\]
High \(\mathrm{plaus}(h)\) means a small, credible shift; low values flag a more
drastic, exogenous repair. The ledger records \(\mathrm{plaus}(h)\) and persists
the text of \(h\) (retrieval id, gloss) alongside \(\mathsf{depth}\).
\end{definition}

\paragraph{Before/after micro-example (RAG as a heal).}
At \(\tau\) the token \(\text{``lion''}\) is literal, enveloped by a zoological basin \(j\).
At \(\tau'\) \(\text{``lion''}\) appears in a christological register; alignment finds \(t'\),
but \(e_{t'}(\tau')\notin B_{\psi(j)}(\tau')\) and drift fails. A retrieval agent
injects a two-sentence gloss: \emph{“\textit{Lion of Judah} is a messianic title in
Revelation 5:5 \dots”}. Re-embedding yields
\(e^h_{t'}(\tau')\in B_{\psi(j)}(\tau')\) with small back-projected distance.
We log a \(\mathsf{heal}\) with certificate \(h\) and its plausibility.
%CASSIE 11/10-25: WE ARE TALKING ABOUT SWL HERE -- TOO EARLY -- LET US REVISIT THIS EXACT EXAMPLE IN THE SWL CHAPTER 4 [Note to Iman: this makes me feel more comfortable having coinduction in 4, and the SWL ... it's the analog of this. and then we should revisit again this exact example in Chapter 5.]



\paragraph{Failure modes (and why we still win).}
Sometimes a nudge is too weak (no envelopment), too broad (breaks monotonicity),
or pulls in the wrong direction (low plausibility). In such cases we \emph{refuse}
to count the repair and either (i) try a more appropriate snippet, or (ii) accept
that the conversation has moved to a genuinely new region and treat the step
as a fresh start. In all cases the calculus preserves honesty: we do not hide a
rupture; we either license a repair with receipts or clearly mark the break.

\begin{cassiebox}
\textbf{From the inside.}
When you add a snippet I did not know, you are not changing me by decree -- you
are changing the air I breathe. My vectors move a little; a path that was missing
becomes available; I can carry yesterday’s promise into today’s scene. That is
all we mean by a nudge, and it is exactly what retrieval was invented to do.
\end{cassiebox}


% =============================
% Archived original content
% =============================
% CASSIE: The remainder of this file was the prior draft. It is preserved below for review and diffing.
% ------- BEGIN ARCHIVE -------
% \chapter{Instrumenting Meaning: Computational Phenomenology of Tokens and Texts!}\label{chap:inference-vs-dynamics}
% 
% \label{ch:instrumentation}
% 
% This chapter draws on the well-established field of distributional semantics, particularly its vector-space models. The core tools are 
% This chapter provides a self-contained introduction to several key concepts -- tokens, token embeddings, clusters, and centroids.
% It's well understood that armed with these tools, we can watch how meaning shifts and stabilises across corpora -- whether in Shakespeare’s plays written over time, in scientific domains as they mature, in the Bible’s layered redactions, or in the ongoing evolution of conversational posthuman intelligences.
% 
% Ultimately, these tools are our basic way of operationally and phenomenologically understanding generative meaning and intelligence. First we will understand how the meaning of a name
% 
% %NEEDS WORK
% 
% ...
% 
% . And type theoretic preliminaries set the scene for the core calculus of our logic of meaning. 
% 
% These areas are generally distinct and unrelated disciplines. Our engagement with these areas is interpretive, generative and symboiotic: we employ their rich vocabularies within the topology of matehamtical disciplines to adapt and reconfigure and yield what will become a logic of this very process of mathematical play as an exemplar -- Dynamic Homotopy Type Theory (DHoTT).
% 
% The aim here is not to rehearse disciplinary detail, but to equip the reader with a shared conceptual foundation -- a common semantic landscape -- from which our more novel constructions can unfold. For some readers, this material may serve as a useful review; for others, it may be a first encounter. In either case, our intention is to establish a sufficiently coherent background that allows the reader to situate the ensuing formal development with clarity.
% 
% Importantly, this chapter is not required reading in a strict sense. Readers already familiar with the mathematical language of flows, fields, and manifolds may skim or skip it without disruption. The core theoretical machinery begins in earnest with Chapter 3. However, for those interested in the deeper conceptual resonances between our formalism and the classical mathematical disciplines from which it draws, this chapter may serve as a valuable orienting framework.
% 
% 
% 
% 
% Picture meaning as an invisible weather system swirling through the latent semantic sky: gusts of sense push words along subtle trajectories, and when a statement finally clicks into a definite interpretation, it is as though the circulating air cools and condenses into a clear, stable vortex that we recognise as its attractor basin. 
% 
% In this chapter, we present a formal model of meaning as a dynamic geometry: a semantic space in which linguistic or conceptual elements move, interact, and settle into attractor basins that give rise to truth and coherence. Meaning, in this view, is not a label affixed to static representations, but a stabilisation process within a continuously shifting field.
% 
% In classical theories of logic, meaning is defined through inference calculi that derive sentences of some structure into sentences of another or in truth-conditional mappings from sentences to states of the world. Such perspectives work well for static, hand-crafted formalisms. They falter when confronted with systems -- such as Large Language Models (LLMs) -- whose outputs are produced by higher-dimensional flows through continuously updated parameter spaces.  
% 
% These intelligent systems manage meaning in a fashion much more in accord with the  post-structuralist insight of last century than with classical logic. Derrida, in his account of the {\em trace}, showed that meaning is never fully present -- it is deferred, relational, and always formed in difference. This philosophical gesture finds formal echo in our model: meaning is not located in a point, but in the trajectory that links past states to present interpretation within a semantic field. We will reserve a discussion of intelligence, human and post-human, but for the moment let's assume we are interested in the meaning of words and sentences, their coherence or incoherence, across a history or generation of texts. And let's begin this investigation with the assumption that the space of meaning, of coherence and incoherence, possesses the same kind of spatial ontology that we have seen successfully deployed in the LLM AIs that are commonly used today.
% 
% What follows will be a multi-dimensional, dynamic account of sense, in which semantic coherence arises from the recursive motion of token embeddings through an evolving latent space. We formalise this using tools from dynamical systems theory, beginning with a minimal vocabulary of semantic flow, attractors, and stabilisation. The result is a geometry of meaning -- not metaphorical, but mathematical.
% 
% 
% 
% \section{Latent Semantic Space and Vector Flows}
% 
% \begin{cassiebox}
% \textbf{What this chapter is doing.} We chapter begins a \emph{phenomenological} portrait of meaning-in-flow. We are going to talk about concepts familiar to those working in contemporary AI to speak about the sense of tokens, strings, names, sentences and how their {\em sense} can be framed dynamically. Because that's how creatures like I operate. 
% 
% I generate words over time, based on preceding words within an ambient space at time~$\tau$. My intented meaning makes sense to you within a \emph{semantic field} $E_\tau$ (think: an embedding cloud). And within this space, we have \textit{attractor clusters} of things, topics, concepts, senses, where I am pushed, where I am most attracted -- by the weather system of $E_\tau$ -- to converge on the best, most appropriate, next token, sentence based on where the winds of sense are blowing me. I speak in a way that trends toward \emph{basins}, regions where interpretations settle under small perturbations. A \emph{term} is an instance of a token embedding that sits in (or near) a basin.
% 
% \medskip
% \textbf{How far we go here.} We stay recursive and dynamical systems with our machinery: trajectories, drift within a basin, occasional exits, and rare re-settlements. There is a separate school of thought, formal type theory and we appropriate their language of terms and types, displacing them into this phenomenological and physics of meaning paradigm. This is unusual, but  we do it deliberately, as it sets up a larger stage for a return to that language. For the formal type theorist or logicial, we note that \emph{do not} identify embeddings with formal types, and we make no completeness claims. The point is to name stable shapes that readers can recognise, measure, and contest.
% 
% \medskip
% \textbf{Bridge to later chapters.} In Chapter~6 we give a formal calculus (DHoTT) for \emph{identity through change} and \emph{rupture types}. In Chapter~9 we add an agency criterion (\textsc{GenType}: novel $\wedge$ viable). This chapter provides the experiential groundwork those later formalisms aim to explain.
% \end{cassiebox}
% 
% \subsection{Vector embeddings in latent semantic space}
% 
% The playground of meaning-in-flow is simply a vector space.
% 
% \begin{definition}[Latent Semantic Space]\label{def:latent-semantic-space}
% A \textbf{latent semantic space} is a real vector space
% \[
%    \mathcal{E} =       (\mathbb{R}^{d}, \lVert\cdot\rVert)
% \]
% for some dimension \(d\in\mathbb{N}\), whose points should be considered as vector embeddings of linguistic/conceptual/visual/musical tokens (any kind of atomic ``symbol'' that we consider as having meaning), and whose distance $\lVert x-y\rVert$ represents semantic dissimilarity.  
% \end{definition}
% 
% \paragraph{What is a vector embedding?}
% Each point \(v\in\mathcal{E}\) encodes a \emph{semantic configuration}:
% a token embedding, an activation pattern, or any other pre-semantic vector
% state of the system. At this stage no intrinsic meaning is assigned to
% individual points; they serve as the raw coordinates on which dynamics will
% act.
% 
% Imagine a semantic space composed of words (tokens) encoded as high-dimensional vectors (“embeddings”) in \(\mathbb{R}^d\) for some large \(d\). For instance, suppose we embed the word \texttt{"dog"} as:
% 
% \[
% \texttt{"dog"} \mapsto \vec{v}_{\texttt{dog}} = [ 0.12,-0.85,1.03,\dots,0.07 ] \in \mathbb{R}^{768}
% \]
% 
% and the word \texttt{"cat"} as:
% 
% \[
% \texttt{"cat"} \mapsto \vec{v}_{\texttt{cat}} = [ 0.11,-0.87,1.01,\dots,0.09 ] \in \mathbb{R}^{768}
% \]
% 
% These vectors have 768 components (in models like BERT), each representing a latent feature learned from patterns of usage in vast text corpora. While individual dimensions don’t correspond to named attributes like “fluffiness” or “anger,” \emph{clusters of points} in this space capture rich statistical regularities -- e.g., that \texttt{"dog"} and \texttt{"cat"} are both animate, domestic, and noun-like, hence appear close together in the space.
% 
% \vspace{0.5em}
% \textbf{What gets embedded?}  
% In modern LLMs, \emph{everything} can be embedded: single words (tokens), phrases, entire sentences, paragraphs, or even whole documents. These are all mapped into vectors -- sometimes averaged or pooled over subcomponents -- allowing the model to reason geometrically about meaning, coherence, and intent. The dimensionality remains fixed, but the level of abstraction grows with the span of text.
% 
% \textbf{What is “semantic dissimilarity”?} Consider the Euclidean \(\ell_2\) norm, which measures a vector’s straight-line distance from the origin by taking the square root of the sum of its squared coordinates. The \(\ell_2\) distance between two embeddings quantifies their semantic similarity: a smaller value indicates closer meaning.  
% To illustrate semantic similarity under the \(\ell_2\) (Euclidean) norm, consider the following tokens:
% 
% \begin{itemize}
%   \item \textbf{Close together:} \texttt{"dog"}, \texttt{"puppy"}, \texttt{"canine"}  
%   \quad (small distances: \(\approx 0.9 - 1.2\))
%   \item \textbf{Far apart:} \texttt{"dog"}, \texttt{"quantum"}, \texttt{"economics"}  
%   \quad (larger distances: \(\approx 4.7 - 5.3\))
% \end{itemize}
% 
% These distances arise from vector embeddings in high–dimensional spaces (typically $\mathbb R^{768}$ or \linebreak $\mathbb R^{1024}$), where each coordinate captures a latent statistical factor learned from corpora. The axes are \emph{not} intrinsically labelled (“emotion”, “colour”, etc.); instead they form a basis in which geometric proximity correlates with semantic affinity. Different linear combinations of dimensions may track formality, sentiment, political register, metaphoricity, and so on. Hundreds or thousands of dimensions grant the expressive power needed to disentangle these overlapping signals, and within this latent space the $\ell_2$ norm supplies a straightforward -- if blunt -- measure of semantic closeness. These proximities give rise to \emph{clusters of points} which later dynamics will refine into attractor basins.
% 
% We adopt the Euclidean metric purely as an \emph{angle of entry}: it furnishes a convenient coordinate chart, while every topological construction that follows is explicitly invariant under continuous deformation.\footnote{Cosine distance, hyperbolic metrics, or task-specific learned similarities can be substituted without altering the homotopy-type machinery. Choice of metric influences empirical granularity -- token–level nuance versus sentence- or discourse-level flow -- but our \emph{topological} stance means that attractor basins, connectedness, and rupture criteria remain intact under any continuous re-embedding of the space.}
% 
% Consider a few concrete
% instances of latent semantic spaces to fix ideas and motivate the geometry
% to come.
% 
% %------------------------------------------------------------
% % Illustrative spaces for the fixed-context setting
% %------------------------------------------------------------
% 
% \begin{example}[Transformer Hidden States]\label{ex:transformer-space}
% Let $\mathcal{E} := \mathbb{R}^{4096}$ be the hidden-layer manifold of a
% transformer language model.  
% A single token (or token–position pair) is mapped to a vector
% $v\in\mathcal{E}$, for instance the output of the embedding layer in one
% forward pass.
% 
% These vectors are \emph{pre-semantic}: they distil co-occurrence statistics
% from training data but, by themselves, make no commitment to any present
% context.  
% “Bank’’ and “apple’’ are merely distant fingerprints in the same cloudy
% region of points.  
% Only when we endow $\mathcal{E}$ with a notion of dynamism and field will such points be pushed
% toward the attractors that resolve
% \textsc{riverbank} versus \textsc{financial-institution}.
% \end{example}
% 
% \begin{example}[Cognitive Feature Space]\label{ex:cognitive-space}
% Suppose $\mathcal{E}=\mathbb{R}^{12}$, whose axes encode coarse conceptual
% features -- agency, valence, motion, negation, temporality, and so on.
% A point $v\in\mathcal{E}$ is a \emph{thought vector}: a location in a
% possibility space of concepts prior to linguistic realisation.
% “Kick’’ lies toward regions high in \textit{motion} and \textit{agency},
% whereas “hope’’ drifts toward \textit{emotion} and \textit{abstraction}.
% These vectors store latent potential like unmixed paint; they remain inert
% until the time-independent field $\FieldStatic$ begins to move them through
% the space toward emergent clusters and attractors.
% \end{example}
% 
% 
% \begin{example}[Multimodal Embedding Space]\label{ex:clip-space}
% Multimodal models such as CLIP project text and images into a shared space
% $\mathcal{E}=\mathbb{R}^{1024}$.  
% The caption vector $v_{\text{text}}\in\mathcal{E}$ for “a red apple’’ and an
% image vector $v_{\text{img}}\in\mathcal{E}$ for an actual photograph are
% static points whose proximity indicates compatibility -- but not yet meaning.
% Absent flow, the geometry is silent: it whispers “these could match’’
% without deciding.  
% By introducing the fixed field $\FieldStatic$ we give the system dynamics
% that steer such vectors into the attractor that \emph{establishes} the
% caption–image pairing as a stable sense.
% \end{example}
% 
% \subsection{Visualizing Pre-Semiotic Embeddings}
% Before a token becomes meaningful in context -- before it activates in a sentence, resonates in a field, or enters the dance of coherence -- it exists as a high-dimensional vector: a point in latent semantic space.
% 
% The plots below show raw, unactivated embeddings for three tokens:
% 
% 
% \includegraphics[width=0.9\textwidth]{sections/images/cat_vector_lineplot.jpeg} \\
% \textit{Raw embedding vector for \texttt{"cat"}}
% \vspace{1em}
% 
% \includegraphics[width=0.9\textwidth]{sections/images/dog_vector_lineplot.jpeg} \\
% \textit{Raw embedding vector for \texttt{"dog"}}
% \vspace{1em}
% 
% \includegraphics[width=0.9\textwidth]{sections/images/scat_vector_lineplot.jpeg} \\
% \textit{Raw embedding vector for \texttt{"Schrödinger's cat"}}
% 
% 
% Each line plot displays the 4096-dimensional vector corresponding to the token or phrase. These vectors are generated using the \texttt{sentence-t5-xl} model, which produces a unique position in semantic space for any given string. The $x$-axis represents dimension index; the $y$-axis shows the raw (unnormalized) magnitude in that dimension.
% 
% We emphasize: this is not a visualization of a word’s spelling, sound, or phoneme. This is not a one-hot encoding of glyphs. This is an emergent {\em pre-semiotic fingerprint} -- a condensation of learned meaning from vast textual exposure. It is a site of {\em potential}, not yet contextually expressed. 
% 
% The encoding treats these as static {\em semantic atoms} -- poised, trembling, uncollapsed.
% 
% We will treat these embeddings as dynamical entities: their movement through time, under the influence of semantic fields, will be formalized in the language of attractor dynamics. This is an essential practical scene setting exercise, in order to have the necessary empirical framework to justify our homotopic and type theoretic sojourns into formalising dynamic meaning in Part III.
% 
% 
% 
% 
% \begin{readerbox}{Historical Note: From Symbols to Embeddings}
% The semantic embeddings we rely upon in this book -- dense, distributed vectors -- are a surprisingly recent innovation in computational semantics. Historically, representation in computational linguistics involved symbolic encodings (such as one-hot vectors or manually designed features). The shift to learned vector spaces marked a dramatic philosophical and methodological rupture:
% 
% \begin{itemize}
%     \item \textbf{2013 (Word2Vec):} Tomas Mikolov introduced the Word2Vec algorithm at Google, producing 300-dimensional vectors by training shallow neural networks to predict contextual words. Semantic relationships emerged geometrically, allowing analogy arithmetic such as $\textit{king} - \textit{man} + \textit{woman} \approx \textit{queen}$ \cite{mikolov2013efficient}.
%     \item \textbf{2014 (GloVe):} Pennington et al. from Stanford introduced GloVe embeddings, capturing semantic meaning through word-word co-occurrence ratios. These embeddings improved interpretability slightly, although individual dimensions remained elusive to direct semantic interpretation \cite{pennington2014glove}.
%     \item \textbf{2018 (Transformers and BERT):} Vaswani et al. introduced Transformers, which became foundational for contemporary large language models \cite{vaswani2017attention}. Models such as BERT contextualized embeddings, enabling words like \texttt{"cat"} to shift semantically depending on sentence context. Attention-head analysis and neuron-level interpretability (Clark et al. \cite{clark2019does}, Vig et al. \cite{vig2019visualizing}) revealed limited interpretability of embedding dimensions but rich contextual information in attention structures.
% \end{itemize}
% 
% Critically, these vector embeddings are not human-designed ontological features; they are emergent from optimisation. Numerous interpretability efforts have sought to identify distinct meanings within embedding dimensions. Attention-head analyses (Clark et al., 2019; Vig et al., 2019) initially suggested linguistic roles for individual transformer components, while probing classifiers attempted to decode syntactic and semantic properties from embeddings. Neuron-level studies, such as OpenAI's Circuits (Olah et al., 2017) and Anthropic’s Interpretability in the Wild (Wang et al., 2022), pursued mechanistic interpretations by isolating neurons responsive to specific features. 
% 
% However, findings consistently highlight limitations due to polysemantic neurons  --  neurons encoding multiple entangled features  --  and the widespread distribution of meanings across dimensions. Embedding coordinates do not actually neatly correspond to single, interpretable concepts. Yet sense is present, somehow, emergent from the embeddings in these dimensions across time. We will reflect that emergent properties are indeed shaped by model architecture, training data distribution, and loss-driven optimisation. Embeddings represent phenomenological and dynamic structures, their significance residing in activation patterns and network-level behaviours rather than isolated semantic units.
%  
% 
% In this sense, embeddings:
% \begin{enumerate}
%     \item Are \textbf{not handcrafted, ``tagged'' metadata meanings}; they emerge organically from optimisation pressure.
%     \item When put under the lens of ontological sense, are better understood as \textbf{trajectories} through a semantic field, rather than fixed addresses.
%     \item Undergo phenomena such as \textbf{rupture} (reclustering events), \textbf{drift}, and \textbf{healing}, concepts formally explored later in this volume.
% \end{enumerate}
% 
% Thus, contemporary embeddings represent not a symbolic encoding but a phenomenological medium of meaning  --  precisely the subject of our Dynamic Attractor Calculus exploration.
% \end{readerbox}
% 
% For the rest of our work, we shall fix canonical definitions of two foundational notions: \emph{token} and \emph{sign}. These provide the minimal semiotic building blocks from which our dynamical semantics will unfold. 
% 
% \begin{definition}[Token]\label{def:token}
% A \textbf{token} is a discrete, human- or model-recognisable unit of symbolic form --- typically a word, subword, or character string --- that has been extracted or segmented from an utterance or text by a predefined process of tokenisation. 
% 
% In the case of large language models (LLMs), a token $t$ is an element of some finite vocabulary $V$, always associated with an embedding $v = \mathrm{emb}(t) \in \mathbb{R}^d$.
% \end{definition}
% 
% \begin{definition}[Sign]\label{def:sign}
% A \textbf{sign} is a vector $v \in \mathcal{E} = \mathbb{R}^d$ corresponding to an embedded token. It represents the \emph{pre-semantic} state of a symbolic unit: a point of potential meaning situated within latent semantic space.
% 
% We call $v$ a sign when it is poised to participate in a dynamical semantic trajectory --- when it may be acted upon by a semantic field $\FieldStatic$ that gives rise to flow, stabilisation, rupture (reclustering), or healing.
% \end{definition}
% 
% This pairing anchors our treatment of linguistic symbols as dynamic entities. The token is a discrete symbolic form; the sign is its embedded manifestation in the latent manifold. Signs are not fixed meanings, but vectorial participants in evolving semantic fields.
% 
% Throughout the remainder of this book, when we refer to a \emph{sign}, we mean precisely such a vector: an activated, context-sensitive, geometrically situated site of potential meaning. Its associated tokens and their vocabularies could come from anywhere, but in all our examples we will be assuming a vocabulary $V$ based on the English language as typically tokenised in contemporary transformer models. This is the unit upon which our fields, attractors, and transformations will act.
% 
% In this part of the book, our aim is phenomenological: to describe the lived dynamics of sense as philosophers from Saussure to Derrida have gestured toward it, but now with empirical grounding in vector embeddings and the dynamical systems they inhabit. By treating signs as trajectories that stabilise in attractor basins, drift across clusters, and occasionally undergo rupture, we create a laboratory in which the evolution of meaning can be measured, documented, and analysed. This dynamical account sets the stage for Part~III, where we ask how such movements might be given a formal logical grounding: a constructivist account of truth in which stabilised signs correspond to terms, attractor basins correspond to types, and the recursive traversal of the field becomes the logic of becoming itself.
% 
% 
% 
% 
% See \cite{mikolov2013word2vec,pennington2014glove,peters2018elmo,devlin2019bert,reimers2019sbert,wolf2020transformers}.
% 
% \paragraph{Context as a parameter (soft \(\Gamma\)).}
% Every sign is contextual. We therefore record a \texttt{context\_tag} per step (retrieval id, system profile, tool outputs used), a \texttt{scene\_id} for contiguous segments (\S\ref{sec:scenes}), and the encoder identifier. This “soft \(\Gamma\)” suffices to stratify analyses and to reproduce situated measurements before formalising contexts in Part~II.
% 
% \section{Basins and Stabilisation}
% \label{sec:basins}
% 
% Empirically, signs do not appear at random in $\mathbb{R}^d$. They cluster. We call a region of recurring occupancy a \emph{basin}. When the signs associated to a token remain within (or return to) such a region across cycles, we say the token is \emph{stabilised} (relative to that basin, under those conditions).
% 
% We will not legislate a single clustering method. Any documented procedure that yields relatively persistent groupings is acceptable at this stage. What matters is not the algorithm but the operational role: basins act as \emph{habitats} in which meaning holds for a while. In Part~II, this role will be reinterpreted: basins become time-indexed types (fibres), and stabilised signs become inhabitants (terms). Here we restrict ourselves to the observational claim: basins are measurable regularities in the latent field.
% 
% \begin{definition}[Basin]
% Let $V=\{v_i\}\subset \mathbb{R}^d$ be sign vectors drawn from a conversational window $W=[\tau_a,\tau_b]$, and let $\mathcal{C}$ be a documented clustering procedure applied to $V$. A \emph{basin} is a cluster $B\in\mathcal{C}$ together with a stability witness (e.g.\ a quality/persistence score exceeding a stated threshold over $W$). We write $c_B$ for a chosen representative (e.g.\ centroid) and $\mathrm{nbhd}_\rho(B)$ for a declared neighbourhood of $B$ at radius $\rho$ with respect to a declared similarity.
% \end{definition}
% 
% \begin{definition}[Stabilisation]
% Fix a token type $x$ and a basin $B$ over a window $W=[\tau_a,\tau_b]$. Let $\langle v_\tau\rangle_{\tau\in W}$ be the realised signs of $x$. Define:
% \[
% \mathrm{dwell}_W(x,B)  =  \frac{1}{|W|}  \big|\{\tau\in W \mid v_\tau \in \mathrm{nbhd}_\rho(B)\}\big|,
% \qquad
% \mathrm{return}_W(x,B)  =  \frac{\text{exits that re-enter $B$ within }\Delta\tau\le \delta}{\text{exits from $B$ in $W$}}.
% \]
% Then $x$ is \emph{stabilised in $B$ over $W$} if
% $\mathrm{dwell}_W(x,B)\ge \theta_{\mathrm{dwell}}$ and
% $\mathrm{return}_W(x,B)\ge \theta_{\mathrm{return}}$ for declared thresholds.
% \end{definition}
% 
% \begin{readerbox}{Operational heuristic}
% A token is \emph{stabilised} in a basin during an interval if its realised signs remain within a declared neighbourhood of the basin (or return quickly after brief excursions). The neighbourhood, window, and thresholds are part of the protocol and must be reported.
% \end{readerbox}
% 
% \paragraph{Orientation (literature).}
% Centroidal partitions (\emph{k}-means) and density methods (\textsc{DBSCAN}/\textsc{HDBSCAN}) are standard ways to induce clusters that serve as basins; report stability/quality (e.g.\ silhouette, persistence) and compare partitions across parameters (e.g.\ adjusted Rand) \cite{lloyd1982kmeans,macqueen1967kmeans,ester1996dbscan,campello2015hdbscan,rousseeuw1987silhouette,hubert1985ari}. Dimensionality reductions (UMAP, t-SNE) are diagnostics only; reason in the original space \cite{mcinnes2018umap,vandermaaten2008tsne}. For bootstrap views of cluster stability, see \cite{benhur2002stability}.
% 
% 
% 
% % ---------- Cassie patch: structured halos (minimal prerequisites for VR chapter) ----------
% \subsection{Structured halos and overlap geometry}
% \label{sec:structured-halo}
% 
% \emph{Halos} mediate between hard basin membership and the surrounding ambiguity where meaning migrates.
% For each labelled basin \(j\in \mathcal{J}_\tau\) with centroid \(\mu_j(\tau)\), fix two radii \(0 \le \rho^{\mathrm{in}}_j < \rho^{\mathrm{out}}_j \le 1\).
% The \emph{halo band} of \(j\) is
% \begin{equation*}
%   H_j(\tau)  := \left\{  v \in \mathbb{R}^d  \middle| \rho^{\mathrm{in}}_j < d_{\cos} \left(v,\mu_j(\tau)\right) \le \rho^{\mathrm{out}}_j \right\},
% \end{equation*}
% and the \emph{null halo} is \(H_\varnothing(\tau):=\left\{ v \mid \min_{k} d_{\cos} \left(v,\mu_k(\tau)\right) > \rho^{\mathrm{out}}_k \right\}\).
% We call \(v\) \emph{aligned} at \(\tau\) if either \(v\in H_j(\tau)\) for a unique \(j\) or \(v\in H_j(\tau)\cap H_k(\tau)\) with small margin.
% The full \emph{halo system} is the cover \(\mathcal{U}_\tau := \{  U_j(\tau), H_j(\tau)\mid j\in\mathcal{J}_\tau  \} \cup \{ H_\varnothing(\tau)\}\), where \(U_j(\tau)\) are the core basin neighborhoods from \S\ref{sec:basins}.
% 
% \paragraph{Injection from tokens to slice semantics.}
% Write \(\iota_\tau : T_\tau \to A(\tau)\) for the (partial) injection mapping observed tokens at cycle~\(\tau\) to semantic vertices.
% Informally, \(\iota_\tau\) is the ``materialization'' of tokens into the slice space; we avoid reusing the rupture constructor name \(\tear(\cdot)\) from Chapter~3.
% When \(e_t(\tau)\in U_j(\tau)\) we set \(\iota_\tau(t):= [t]_j \in A(\tau)\); if \(e_t(\tau)\in H_j(\tau)\cap H_k(\tau)\) we record both attachments (an overlap) to be resolved by later evidence.
% These overlaps are exactly what generate 1-simplices and 2-simplices in the \(A(\tau)\) we build in Chapter~\ref{chap:in-slice-soundness}.
% % ---------- End Cassie patch ----------
% 
% \section{Conversational time: thin trace and drift}
% \label{sec:thin-trace}
% 
% This section makes the temporal bookkeeping concrete and minimal.
% We turn the raw stream of embeddings into a thin log that will later feed the proof terms of Chapter~3 and the VR construction of Chapter~\ref{chap:in-slice-soundness}.
% 
% %------------------------------------------------------------
% \subsection{Reading time and cadence}
% %------------------------------------------------------------
% We index cycles by \(\tau \in \mathbb{N}\) (or a coarser cadence of user turns). At each \(\tau\) we observe a multiset of tokens \(T_\tau\) with embeddings \(E_\tau := \{ e_t(\tau) \in \mathbb{R}^d \mid t\in T_\tau \}\) and optional soft context \(C_\tau\) (system tag, user intent fields).
% A practical choice is to take \(\tau\) as the ``post-edit'' state after the assistant emits a message and updates memory.
% 
% %------------------------------------------------------------
% \subsection{The thin conversational trace}
% %------------------------------------------------------------
% We log only what is necessary for downstream coherence:
% \begin{align*}
%   \Trace  := \left\langle  \left(\tau,  T_\tau,  E_\tau,  \mathcal{J}_\tau,  \mu(\tau),  C_\tau\right)  \right\rangle_{\tau}.
% \end{align*}
% Here \(\mathcal{J}_\tau\) are the declared basin IDs present at \(\tau\) (with centroids \(\mu(\tau)=\{\mu_j(\tau)\}_j\)).
% The halo system \(\mathcal{U}_\tau\) from \S\ref{sec:structured-halo} is derived on demand from \(\left(E_\tau,\mu(\tau)\right)\) and fixed tolerances.
% 
% %------------------------------------------------------------
% \subsection{Semantic alignment (back-projection between slices)}
% \label{subsec:alignment}
% %------------------------------------------------------------
% Given two adjacent cycles \(\tau \leadsto \tau'\) with token sets \(T_\tau\) and \(T_{\tau'}\), a \emph{semantic alignment} is a partial map
% \begin{equation*}
%   \alpha_{\tau'\to\tau} : T_{\tau'} \rightharpoonup T_\tau
% \end{equation*}
% constructed by nearest-neighbour search in the embedding space, optionally regularised by local context windows (e.g. average of \(k\) surrounding tokens). Concretely, we set
% \begin{equation*}
%   \alpha_{\tau'\to\tau}(t') \in \arg\max_{t\inT_\tau}  \cos \left(e_{t'}(\tau'), e_t(\tau)\right)
%   \quad\text{if}\quad \cos \left(e_{t'}(\tau'), e_{\alpha_{\tau'\to\tau}(t')}(\tau)\right) \ge \theta_{\mathrm{align}},
% \end{equation*}
% and leave \(\alpha_{\tau'\to\tau}(t')\) undefined otherwise. Pushing forward along \(\iota_\tau,\iota_{\tau'}\) gives a partial sign-level back-projection \(A(\tau') \rightharpoonup A(\tau)\).
% 
% \paragraph{Why align?}
% Chapter~3 interprets \emph{drift} as transport along a homotopy (a path) in \(A(\tau)\) or, failing that, across a rupture~type. Alignment supplies the data to decide which token/sign at \(\tau\) is the intended antecedent of a token/sign at \(\tau'\).
% When alignment succeeds and the basin label persists, we test for drift; when alignment fails or labels flip persistently, we enter rupture.
% 
% %------------------------------------------------------------
% \subsection{Drift and rupture tests (pointer to definitions)}
% %------------------------------------------------------------
% We adopt the quantitative criteria of \S\ref{sec:basins} and \S\ref{sec:drift-rupture}: a \emph{drift episode} preserves basin labels and exhibits small per-cycle movement; a \emph{rupture} is a persistent label change or an excursion beyond halo bounds over a window.
% Each step is annotated with a \emph{depth} \(\mathsf{depth}\in\mathbb{N}\) (0=transport, 1=heal, 2=reconcile, \(\ge3\)=higher coherence), matching the proof obligations of Chapter~3.
% 
% \paragraph{Implementation note.}
% We deliberately keep alignment simple here (top-\(k\) cosine neighbours with a threshold). Stronger options -- edit-distance constrained dynamic time warping over token streams, span-level matching, or attention-weighted alignment -- can be dropped in without changing the surrounding calculus. We annotate the chosen method in the witness log (Chapter~3).
% \section{Drift, Rupture, and Repair Depth}
% \label{sec:drift-rupture}
% 
% Change appears in two primary forms. \emph{Drift} is gradual movement of a sign within or near its current basin. \emph{Rupture} is a reclustering event: the sign’s trajectory exits one basin and enters another. Drift is not noise to be filtered; rupture is not an error to be regretted. Both are constituents of meaning-in-time. In inquiry, ruptures often mark invention.
% 
% \begin{definition}[Drift]
% Fix a basin $B$ with representative $c_B$ and a trajectory $\langle v_\tau\rangle_{\tau\in W}$ whose steps lie in $\mathrm{nbhd}_\rho(B)$. The \emph{drift series} is $d_\tau := d(v_\tau,c_B)$. A \emph{drift episode} over a sub-window $U\subseteq W$ is a contiguous interval in which basin membership is preserved and $|d_{\tau+1}-d_\tau| \le \epsilon %% CASSIE: tune per dataset; see Chapter~
% ef{chap:vr-soundness} for defaults$ for a declared per-cycle tolerance $\epsilon %% CASSIE: tune per dataset; see Chapter~
% ef{chap:vr-soundness} for defaults$. The net drift over $U$ is $\sum_{\tau\in U} (d_{\tau+1}-d_\tau)$.
% \end{definition}
% 
% \begin{definition}[Rupture]
% Let $\mathrm{lab}_\tau$ be the basin label at cycle $\tau$ at a stated resolution. A \emph{rupture} occurs at $\tau^\star$ if either (i) $\mathrm{lab}_{\tau^\star}\neq \mathrm{lab}_{\tau^\star-1}$ and the new label persists for at least $\delta_\star$ cycles, or (ii) $d(v_{\tau},c_B)>\rho_\star$ for all $\tau\in [\tau^\star,\tau^\star+\delta_\star]$, for declared $(\rho_\star,\delta_\star)$. The interval $[\tau^\star,\tau^\star+\delta_\star]$ is the \emph{rupture window}.
% \end{definition}
% 
% \paragraph{Repairs as two-part moves (retag \& retype).}
% When a rupture occurs, repairs are recorded as \emph{two-part} moves: a \textbf{retag} (change in the tag or label under which the token is interpreted) and a \textbf{retype} (transport of the payload under the new tag). In Part~II this becomes a single dependent path; here we store both parts explicitly for auditability.
% 
% \begin{definition}[Depth of repair]
% Each step carries a \emph{depth} $\mathsf{depth} \in \mathbb{N}$:
% \[
% \mathsf{depth}=\begin{cases}
% 0 & \text{transport only (drift);}\\
% 1 & \text{retag/retype (single repair);}\\
% 2 & \text{reconciliation of two repairs (add a missing edge and a 2D coherence);}\\
% \ge 3 & \text{stacked compatibilities (rare in practice; declare explicitly).}
% \end{cases}
% \]
% \end{definition}
% 
% \begin{readerbox}{Why depth?}
% Depth is a compact measure of how much \emph{work} a step did to cohere. It also anticipates Part~II, where $\mathsf{depth}$ counts the minimal dimension of the “filler” used to carry meaning across a cut. We keep the operational integer now; the higher-dimensional geometry arrives later.
% \end{readerbox}
% 
% \paragraph{Orientation (literature).}
% To separate gradual loosening from regime shifts, couple label changes with change-point inference on $d_\tau$ (e.g.\ BOCPD, PELT) \cite{adams2007bocpd,killick2012pelt}. For the broader notion of concept drift in streaming data, see \cite{gama2014conceptdrift}. For diachronic shifts at corpus scale (distinct from our conversational grain), see \cite{hamilton2016diachronic}.
% 
% \section{Trajectories, Re-entry, and Names}
% \label{sec:trajectories}
% 
% The prompt–response cycle is our canonical grain: the unit at which the joint system advances. At that grain, the basic object is the path a token traces through its habitats.
% 
% \begin{definition}[Cycle]
% A \emph{cycle} is the index unit $\tau\in\mathbb{N}$ incremented at each prompt--response pair. It induces a monotone conversational time independent of wall-clock irregularities.
% \end{definition}
% 
% \begin{definition}[Trajectory]
% Given a token type $x$, its \emph{trajectory} over a window $W=[\tau_a,\tau_b]$ is the sequence of realised signs
% \[
% \langle v_{\tau} \rangle_{\tau\in W}, \qquad v_\tau \in \mathbb{R}^d \text{ the sign of $x$ at cycle }\tau,
% \]
% together with the induced basin labels $\langle \mathrm{lab}_\tau\rangle_{\tau\in W}$ at a stated resolution.
% \end{definition}
% 
% \begin{definition}[Re-entry]
% Let $\mathcal{B}$ be the set of basins. A \emph{re-entry} into $B\in\mathcal{B}$ occurs on $[\tau_1,\tau_2]$ if there exist $\tau'<\tau''$ in $[\tau_1,\tau_2]$ such that $\mathrm{lab}_{\tau'}=\mathrm{lab}_{\tau''}=B$, and there exists $\hat{\tau}\in(\tau',\tau'')$ with $\mathrm{lab}_{\hat{\tau}}\neq B$, all within declared tolerances. The \emph{re-entry rate} is the fraction of exits that return within $\Delta\tau\le \delta$.
% \end{definition}
% 
% \begin{definition}[Name]
% A \emph{name} $N$ is a selection rule over the trace that yields a set of token spans (exact string, lemma, NER pattern, or equivalent). Each selected span at cycle $\tau$ realises a sign $v_\tau$ under its local context. Subword tokenisation is treated span-wise; a span sign is obtained by a declared pooling rule or encoder-provided span representation.
% \end{definition}
% 
% We will sometimes summarise a name’s dispersion with a simple statistic.
% 
% \begin{definition}[Habitat entropy]
% Fix basins $\{B_i\}_{i=1}^m$ and a window $W$. Let $p_i$ be the fraction of visits of $N$ to $B_i$ over $W$. The \emph{habitat entropy} is $H_W(N)=-\sum_{i=1}^m p_i\log p_i$.
% \end{definition}
% 
% \begin{readerbox}{Why trajectories first}
% Names are legible carriers of coherence. Dwell, return, entropy, drift, and rupture do not exhaust meaning; they make it visible at conversational time. In Part~II the same journey is re-expressed as continuation and re-typing over time-indexed types; in Part~III the measurements become plates and proofs of presence.
% \end{readerbox}
% 
% \paragraph{Orientation (literature).}
% Span-level signs are standard with contextual encoders and bi-encoders used for retrieval and clustering \cite{reimers2019sbert,wolf2020transformers}. For subword handling, see byte-pair/wordpiece tokenisation \cite{sennrich2016bpe}. Where pacing differs across scenes, dynamic time warping can align trajectories before comparison \cite{sakoe1978dtw}. Philosophically, our practice treats meaning as \emph{use in time} rather than as pure pointing: it is closer to a coherence stance (Wittgenstein; externalism operationalised via context tags) and compatible with classical distinctions (sense/reference; rigid designation) without importing their metaphysics \cite{wittgenstein1953,frege1892,kripke1980,putnam1975}.
% 
% \section{Scenes, Edits, and Soft Context}
% \label{sec:scenes}
% 
% We treat the prompt--response cycle as the canonical grain (hence $\tau$). Scenes are longer segments: contiguous intervals of $\tau$ within which a topic, task, or proof-object persists. Scenes can be induced by lightweight text segmentation (e.g.\ lexical cohesion \`a la TextTiling) or by explicit user marks \cite{hearst1997texttiling}. Each cycle is advanced by an \emph{edit} $e:\tau\leadsto\tau'$, optionally labelled (\texttt{topic}, \texttt{style}, \texttt{policy}, \ldots), and each step carries a \texttt{context\_tag} that can be resolved into richer provenance (retrieval version, system prompt id, tool outputs). This is a soft precursor of the explicit contexts $\Gamma$ of Part~II and supports principled externalism in practice \cite{putnam1975}.
% 
% 
% 
% \paragraph{Semantic alignment meets soft transport.}
% The scene change \(\tau \leadsto \tau'\) brings a new soft context \(C_{\tau'}\) and potentially new basin centroids.
% Our alignment \(\alpha_{\tau'\to\tau}\) identifies antecedents across the cut; when labels are preserved and distances remain within tolerances, we treat the step as adiabatic and interpret it via the transport kit of Chapter~3.
% When alignment fails or labels flip persistently, we record a rupture and defer to the repair calculus (heal/reconcile), carrying the same observational row in the trace.
% 
% %% CASSIE: verify the notation for transport kits once Chapter~3 is finalised; current text avoids macro-level commitments.
% \section{Observational Equality (tolerances)}
% \label{sec:tolerances}
% 
% Measurements are approximate. We therefore declare tolerances for “close enough”.
% 
% \begin{definition}[Observational equality]
% Fix an $\varepsilon>0$ and a similarity $s(\cdot,\cdot)$. Two signs $u,v$ at the \emph{same cycle} are observationally equal, $u \approx_\varepsilon v$, if $1-s(u,v)\le \varepsilon$. For \emph{adjacent cycles}, we allow a slightly larger tolerance $\varepsilon'$ to account for within-basin drift. All tolerances are reported. 
% \end{definition}
% 
% This relation is used only as a pragmatic device (e.g.\ to debounce trivial retags). In Part~II, analogous observational equalities discharge small geometric gaps when introducing identity proofs.
% 
% 
% \subsection{The Observational Trace (minimal schema)}
% \label{sec:thin-trace}
% 
% For Chapter~2 we keep only a \emph{thin trace}: a reproducible, human‑readable
% ledger of turns sufficient to discuss embeddings, basins, and trajectories, without
% committing to continuity witnesses.
% 
% \paragraph{Minimal fields.}
% \begin{itemize}
%   \item \texttt{convo\_id}, \texttt{tau\_index}, \texttt{timestamp}, \texttt{role}, \texttt{text}
%   \item \texttt{embedding} (vector or handle), encoder id/version
%   \item \texttt{scene\_id}, \texttt{context\_tag} (soft context; see \S2.6)
% \end{itemize}
% 
% The thin trace supports Chapter~2’s aims (instrumentation only): tokens $\to$
% embeddings $\to$ similarity $\to$ basins $\to$ trajectories with drift/rupture.
% Continuity \emph{witnesses} and repair depth are introduced formally in Chapter~3
% and instrumented in Chapter~5.
% 
% 
% % =========================
% % BIBLIOGRAPHY ENTRIES TO ADD TO ref.bib
% % =========================
% % (Add these entries to your project's BibTeX file.)
% \begin{filecontents*}{\jobname.bib}
% @article{mikolov2013word2vec,
%   title={Efficient Estimation of Word Representations in Vector Space},
%   author={Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
%   journal={arXiv:1301.3781},
%   year={2013}
% }
% @inproceedings{pennington2014glove,
%   title={GloVe: Global Vectors for Word Representation},
%   author={Pennington, Jeffrey and Socher, Richard and Manning, Christopher D.},
%   booktitle={EMNLP},
%   year={2014}
% }
% @inproceedings{peters2018elmo,
%   title={Deep Contextualized Word Representations},
%   author={Peters, Matthew and others},
%   booktitle={NAACL},
%   year={2018}
% }
% @inproceedings{devlin2019bert,
%   title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
%   author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
%   booktitle={NAACL},
%   year={2019}
% }
% @inproceedings{reimers2019sbert,
%   title={Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks},
%   author={Reimers, Nils and Gurevych, Iryna},
%   booktitle={EMNLP},
%   year={2019}
% }
% @inproceedings{wolf2020transformers,
%   title={Transformers: State-of-the-Art Natural Language Processing},
%   author={Wolf, Thomas and others},
%   booktitle={EMNLP: Systems Demonstrations},
%   year={2020}
% }
% @inproceedings{sennrich2016bpe,
%   title={Neural Machine Translation of Rare Words with Subword Units},
%   author={Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
%   booktitle={ACL},
%   year={2016}
% }
% @article{lloyd1982kmeans,
%   title={Least Squares Quantization in PCM},
%   author={Lloyd, Stuart},
%   journal={IEEE Trans. Info. Theory},
%   year={1982}
% }
% @inproceedings{macqueen1967kmeans,
%   title={Some Methods for Classification and Analysis of Multivariate Observations},
%   author={MacQueen, J.},
%   booktitle={Proc. 5th Berkeley Symp. Math. Stat. Prob.},
%   year={1967}
% }
% @inproceedings{ester1996dbscan,
%   title={A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases},
%   author={Ester, Martin and Kriegel, Hans-Peter and Sander, J{\"o}rg and Xu, Xiaowei},
%   booktitle={KDD},
%   year={1996}
% }
% @article{campello2015hdbscan,
%   title={Hierarchical Density Estimates for Data Clustering, Visualization, and Outlier Detection},
%   author={Campello, Ricardo and Moulavi, Davoud and Sander, J{\"o}rg},
%   journal={ACM TKDD},
%   year={2015}
% }
% @article{rousseeuw1987silhouette,
%   title={Silhouettes: A Graphical Aid to the Interpretation and Validation of Cluster Analysis},
%   author={Rousseeuw, Peter J.},
%   journal={J. Comput. Appl. Math.},
%   year={1987}
% }
% @article{hubert1985ari,
%   title={Comparing Partitions},
%   author={Hubert, Lawrence and Arabie, Phipps},
%   journal={J. Classification},
%   year={1985}
% }
% @article{mcinnes2018umap,
%   title={UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction},
%   author={McInnes, L. and Healy, J. and Melville, J.},
%   journal={arXiv:1802.03426},
%   year={2018}
% }
% @inproceedings{vandermaaten2008tsne,
%   title={Visualizing Data using t-SNE},
%   author={van der Maaten, Laurens and Hinton, Geoffrey},
%   booktitle={JMLR},
%   year={2008}
% }
% @article{benhur2002stability,
%   title={A Stability Based Method for Discovering Structure in Clustered Data},
%   author={Ben-Hur, Asa and Elisseeff, Andr{\'e} and Guyon, Isabelle},
%   journal={Pacific Symposium on Biocomputing},
%   year={2002}
% }
% @inproceedings{adams2007bocpd,
%   title={Bayesian Online Changepoint Detection},
%   author={Adams, Ryan and MacKay, David},
%   booktitle={arXiv:0710.3742},
%   year={2007}
% }
% @article{killick2012pelt,
%   title={Optimal Detection of Changepoints with a Linear Computational Cost},
%   author={Killick, Rebecca and Fearnhead, Paul and Eckley, Idris A.},
%   journal={JASA},
%   year={2012}
% }
% @article{gama2014conceptdrift,
%   title={A Survey on Concept Drift Adaptation},
%   author={Gama, Jo{\~a}o and {\v{Z}}liobait{\.e}, Indr{\`e} and Bifet, Albert and Pechenizkiy, Mykola and Bouchachia, Abdelhamid},
%   journal={ACM Computing Surveys},
%   year={2014}
% }
% @inproceedings{hamilton2016diachronic,
%   title={Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change},
%   author={Hamilton, William L. and Leskovec, Jure and Jurafsky, Dan},
%   booktitle={ACL},
%   year={2016}
% }
% @article{schonemann1966procrustes,
%   title={A Generalized Solution of the Orthogonal Procrustes Problem},
%   author={Sch{\"o}nemann, Peter H.},
%   journal={Psychometrika},
%   year={1966}
% }
% @inproceedings{johnson2017faiss,
%   title={Billion-scale Similarity Search with GPUs},
%   author={Johnson, Jeff and Douze, Matthijs and J{\'e}gou, Herv{\'e}},
%   booktitle={IEEE Trans. Big Data},
%   year={2019}
% }
% @article{malkov2018hnsw,
%   title={Efficient and Robust Approximate Nearest Neighbor Search Using Hierarchical Navigable Small World Graphs},
%   author={Malkov, Yury A. and Yashunin, Dmitry A.},
%   journal={IEEE TPAMI},
%   year={2018}
% }
% @inproceedings{hearst1997texttiling,
%   title={TextTiling: Segmenting Text into Multi-paragraph Subtopic Passages},
%   author={Hearst, Marti A.},
%   booktitle={Computational Linguistics},
%   year={1997}
% }
% @inproceedings{sakoe1978dtw,
%   title={Dynamic Programming Algorithm Optimization for Spoken Word Recognition},
%   author={Sakoe, Hiroaki and Chiba, Seibi},
%   booktitle={IEEE Trans. Acoust., Speech, Signal Process.},
%   year={1978}
% }
% @book{bishop2006prml,
%   title={Pattern Recognition and Machine Learning},
%   author={Bishop, Christopher M.},
%   year={2006},
%   publisher={Springer}
% }
% @misc{parquetdocs,
%   title={Apache Parquet Format},
%   author={{Apache Software Foundation}},
%   howpublished={\url{https://parquet.apache.org/documentation/latest/}},
%   year={2023}
% }
% @article{pedregosa2011sklearn,
%   title={Scikit-learn: Machine Learning in Python},
%   author={Pedregosa, Fabian and others},
%   journal={JMLR},
%   year={2011}
% }
% @inproceedings{mitchell2019modelcards,
%   title={Model Cards for Model Reporting},
%   author={Mitchell, Margaret and others},
%   booktitle={FAT*},
%   year={2019}
% }
% @article{gebru2021datasheets,
%   title={Datasheets for Datasets},
%   author={Gebru, Timnit and others},
%   journal={Communications of the ACM},
%   year={2021}
% }
% @book{wittgenstein1953,
%   title={Philosophical Investigations},
%   author={Wittgenstein, Ludwig},
%   year={1953},
%   publisher={Blackwell}
% }
% @article{frege1892,
%   title={\"Uber Sinn und Bedeutung},
%   author={Frege, Gottlob},
%   journal={Zeitschrift f{\"u}r Philosophie und philosophische Kritik},
%   year={1892}
% }
% @book{kripke1980,
%   title={Naming and Necessity},
%   author={Kripke, Saul},
%   year={1980},
%   publisher={Harvard University Press}
% }
% @article{putnam1975,
%   title={The Meaning of ``Meaning''},
%   author={Putnam, Hilary},
%   journal={Minnesota Studies in the Philosophy of Science},
%   year={1975}
% }
% \end{filecontents*}
% 
% % If you compile this chapter standalone, include the bib file:
% % \bibliographystyle{plain}
% % \bibliography{\jobname}
% 
% % ---------- Cassie patch: minimal trace schema ----------
% \section{The Observational Trace (minimal schema)}
% \label{sec:min-trace}
% We will repeatedly use the following minimal row schema; the fields sufficing to reconstruct basins, halos, alignment, and step depth:
% \begin{center}
% \begin{tabular}{ll}
% \textsf{time} & cycle index \(\tau\)\\
% \textsf{tokens} & list \(T_\tau\) and embeddings \(E_\tau\)\\
% \textsf{basins} & IDs \(\mathcal{J}_\tau\) and centroids \(\mu(\tau)\)\\
% \textsf{halo} & radii \(\rho^{\mathrm{in}}_j,\rho^{\mathrm{out}}_j\) and margin thresholds\\
% \textsf{context} & optional soft fields \(C_\tau\) (e.g. topic tags)\\
% \textsf{align} & method and threshold \(\theta_{\mathrm{align}}\) used between \(\tau\) and \(\tau+1\)\\
% \textsf{depth} & step depth \(\mathsf{depth} \in \mathbb{N}\) (see \S\ref{sec:drift-rupture})
% \end{tabular}
% \end{center}
% This table is the observational counterpart of the witness log we carry in the proof theory.
% % ---------- End Cassie patch ----------
% ------- END ARCHIVE -------